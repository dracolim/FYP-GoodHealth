{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT ANALYSIS TO QA PIPELINE\n",
    "# to find out the reason for a sentiment\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis and question answering models\n",
    "sa_model = pipeline(\"sentiment-analysis\")\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "\n",
    "# Review to analyze\n",
    "review = \"I had a great experience with this product. The customer service was excellent and the product exceeded my expectations.\"\n",
    "\n",
    "# Perform sentiment analysis on the review\n",
    "sentiment = sa_model(review)[0]['label']\n",
    "\n",
    "# Extract key phrases from the review\n",
    "key_phrases = ['customer service', 'product', 'expectations']\n",
    "\n",
    "# Generate questions based on the sentiment and key phrases\n",
    "if sentiment == 'POSITIVE':\n",
    "    questions = ['What made the customer service excellent?',\n",
    "                 'What features of the product exceeded your expectations?']\n",
    "elif sentiment == 'NEGATIVE':\n",
    "    questions = ['What issues did you encounter with the customer service?',\n",
    "                 'What aspects of the product were disappointing?']\n",
    "else:\n",
    "    questions = ['What can you say about the customer service?',\n",
    "                 'What can you say about the product?']\n",
    "\n",
    "# Use QA to answer the questions\n",
    "answers = []\n",
    "for question in questions:\n",
    "    answer = qa_model(question=question, context=review)\n",
    "    answers.append(answer['answer'])\n",
    "\n",
    "# Print the sentiment and answers\n",
    "print('Sentiment:', sentiment)\n",
    "print('Answers:', answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC MODELING\n",
    "\n",
    "!pip install sentence-transformers  # Install the required library\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load BioBERT model\n",
    "biobert = SentenceTransformer('gsarti/biobert-nli')\n",
    "\n",
    "# Load the text data\n",
    "df = pd.read_csv('path/to/data.csv')\n",
    "\n",
    "# Convert the text data to sentence embeddings\n",
    "sentences = df['text'].tolist()\n",
    "embeddings = biobert.encode(sentences)\n",
    "\n",
    "# Perform topic modeling using Latent Dirichlet Allocation\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(embeddings)\n",
    "\n",
    "# Print the topics and the top words in each topic\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print('Topic %d:' % (idx))\n",
    "    print(' '.join([biobert.decode([feature]) for feature in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WongQiHuiYve\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# KEYWORD EXTRACTION\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input \n",
    "      \"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "print(type(distances))\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(len(distances[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Assume that documents is a list of text documents\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Initialize the NMF object with 100 components\n",
    "nmf = NMF(n_components=100, init='nndsvd', max_iter=200)\n",
    "\n",
    "# Factorize the term-document matrix into a word-topic matrix and a topic-document matrix\n",
    "W = nmf.fit_transform(tfidf)\n",
    "\n",
    "# The resulting word embeddings are the rows of the word-topic matrix\n",
    "word_embeddings = W.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
