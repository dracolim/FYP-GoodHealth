{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT ANALYSIS TO QA PIPELINE\n",
    "# to find out the reason for a sentiment\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis and question answering models\n",
    "sa_model = pipeline(\"sentiment-analysis\")\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "\n",
    "# Review to analyze\n",
    "review = \"I had a great experience with this product. The customer service was excellent and the product exceeded my expectations.\"\n",
    "\n",
    "# Perform sentiment analysis on the review\n",
    "sentiment = sa_model(review)[0]['label']\n",
    "\n",
    "# Extract key phrases from the review\n",
    "key_phrases = ['customer service', 'product', 'expectations']\n",
    "\n",
    "# Generate questions based on the sentiment and key phrases\n",
    "if sentiment == 'POSITIVE':\n",
    "    questions = ['What made the customer service excellent?',\n",
    "                 'What features of the product exceeded your expectations?']\n",
    "elif sentiment == 'NEGATIVE':\n",
    "    questions = ['What issues did you encounter with the customer service?',\n",
    "                 'What aspects of the product were disappointing?']\n",
    "else:\n",
    "    questions = ['What can you say about the customer service?',\n",
    "                 'What can you say about the product?']\n",
    "\n",
    "# Use QA to answer the questions\n",
    "answers = []\n",
    "for question in questions:\n",
    "    answer = qa_model(question=question, context=review)\n",
    "    answers.append(answer['answer'])\n",
    "\n",
    "# Print the sentiment and answers\n",
    "print('Sentiment:', sentiment)\n",
    "print('Answers:', answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC MODELING\n",
    "\n",
    "!pip install sentence-transformers  # Install the required library\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load BioBERT model\n",
    "biobert = SentenceTransformer('gsarti/biobert-nli')\n",
    "\n",
    "# Load the text data\n",
    "df = pd.read_csv('path/to/data.csv')\n",
    "\n",
    "# Convert the text data to sentence embeddings\n",
    "sentences = df['text'].tolist()\n",
    "embeddings = biobert.encode(sentences)\n",
    "\n",
    "# Perform topic modeling using Latent Dirichlet Allocation\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(embeddings)\n",
    "\n",
    "# Print the topics and the top words in each topic\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print('Topic %d:' % (idx))\n",
    "    print(' '.join([biobert.decode([feature]) for feature in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WongQiHuiYve\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# KEYWORD EXTRACTION\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input \n",
    "      \"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "print(type(distances))\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(len(distances[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Assume that documents is a list of text documents\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Initialize the NMF object with 100 components\n",
    "nmf = NMF(n_components=100, init='nndsvd', max_iter=200)\n",
    "\n",
    "# Factorize the term-document matrix into a word-topic matrix and a topic-document matrix\n",
    "W = nmf.fit_transform(tfidf)\n",
    "\n",
    "# The resulting word embeddings are the rows of the word-topic matrix\n",
    "word_embeddings = W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text = \"Good night ðŸ˜Š\"\n",
    "# text = preprocess(text)\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n",
    "# scores = output[0][0].detach().numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Good night ðŸ˜Š\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "# ranking = np.argsort(scores)\n",
    "# ranking = ranking[::-1]\n",
    "# for i in range(scores.shape[0]):\n",
    "#     l = labels[ranking[i]]\n",
    "#     s = scores[ranking[i]]\n",
    "#     print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a popular technique in data analysis used to reduce the dimensionality of a dataset while retaining as much of the original variation as possible. Here are some situations where PCA can be helpful:\n",
    "\n",
    "1. Data Visualization: PCA can help in visualizing high-dimensional data by projecting it into a lower dimensional space. This makes it easier to visualize the data and identify patterns and relationships.\n",
    "\n",
    "2. Feature Selection: PCA can be used to identify the most important features in a dataset. By reducing the dimensionality of the data, we can identify the features that explain the most variation in the data.\n",
    "\n",
    "3. Data Preprocessing: PCA can be used as a preprocessing step to remove correlated features from the dataset. This can help to reduce overfitting and improve the performance of a machine learning model.\n",
    "\n",
    "4. Data Compression: PCA can be used to compress data, making it easier to store and process. By reducing the dimensionality of the data, we can save storage space and processing time.\n",
    "\n",
    "5. Noise Reduction: PCA can help to remove noise from a dataset. By reducing the dimensionality of the data, we can identify the most important patterns and relationships while ignoring noise and outliers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
