{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comments=pd.read_excel('evaluations_overall_comments.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Target:                      Evaluator:       Rotation Dates:  \\\n",
      "0     Eliza Chen      Dr. Ong, Andrew Ming Liang    7/1/2022-7/31/2022   \n",
      "1     Eliza Chen         Dr. Chan, Webber Pak Wo    7/1/2022-7/31/2022   \n",
      "2     Eliza Chen              Dr. Lim, Chee Hooi    7/1/2022-7/31/2022   \n",
      "3     Eliza Chen         Dr. Chang, Jason Pik Eu    8/1/2022-8/31/2022   \n",
      "4     Eliza Chen              Dr. Tan, Chee Kiat    8/1/2022-8/31/2022   \n",
      "..           ...                             ...                   ...   \n",
      "163  Rachel Yeap     Dr. Tan, Malcolm Teck Kiang    7/1/2022-7/31/2022   \n",
      "164  Rachel Yeap  Dr. Khor, Christopher Jen Lock    8/1/2022-8/31/2022   \n",
      "165  Rachel Yeap               Dr. Liou, Wei Lun    8/1/2022-8/31/2022   \n",
      "166  Rachel Yeap             Dr. Loo, Khang Ning    9/1/2022-10/2/2022   \n",
      "167  Rachel Yeap        Dr. Kwek, Andrew Boon Eu  10/3/2022-10/31/2022   \n",
      "\n",
      "                         Service:  \\\n",
      "0                             - -   \n",
      "1       SHS-GASTRO:BASIC ENDO-SGH   \n",
      "2       SHS-GASTRO:BASIC ENDO-SGH   \n",
      "3       SHS-GASTRO:BASIC ENDO-SGH   \n",
      "4       SHS-GASTRO:BASIC ENDO-SGH   \n",
      "..                            ...   \n",
      "163  SHS-GASTRO:GEN GASTROHEP-SGH   \n",
      "164  SHS-GASTRO:GEN GASTROHEP-SGH   \n",
      "165  SHS-GASTRO:GEN GASTROHEP-SGH   \n",
      "166            SHS-IM:GEN MED-SGH   \n",
      "167       SHS-GASTRO:ADV ENDO-CGH   \n",
      "\n",
      "                                               Answer:  \\\n",
      "0    Eliza Chen is starting out and therefore as ex...   \n",
      "1    As a new senior resident to gastroenterology, ...   \n",
      "2                                           No concern   \n",
      "3    Overall I would rate Eliza Chen highly as a SR...   \n",
      "4    Eliza Chen has a quiet disposition and gives g...   \n",
      "..                                                 ...   \n",
      "163  did a week of bleeder call, good mgmt plans an...   \n",
      "164  very competent, extremely professional, and a ...   \n",
      "165  No issue. Making good progress compared to fir...   \n",
      "166  Rachel is clinically competent and I can trust...   \n",
      "167  Able to summarise key issues and suggest a rea...   \n",
      "\n",
      "                                             processed  \\\n",
      "0    Eliza Chen is starting out and therefore as ex...   \n",
      "1    As a new senior resident to gastroenterology, ...   \n",
      "2                                           No concern   \n",
      "3    Overall I would rate Eliza Chen highly as a SR...   \n",
      "4    Eliza Chen has a quiet disposition and gives g...   \n",
      "..                                                 ...   \n",
      "163  did a week of bleeder call, good mgmt plans an...   \n",
      "164  very competent, extremely professional, and a ...   \n",
      "165  No issue. Making good progress compared to fir...   \n",
      "166  Rachel is clinically competent and I can trust...   \n",
      "167  Able to summarise key issues and suggest a rea...   \n",
      "\n",
      "                         encoded  \\\n",
      "0    [input_ids, attention_mask]   \n",
      "1    [input_ids, attention_mask]   \n",
      "2    [input_ids, attention_mask]   \n",
      "3    [input_ids, attention_mask]   \n",
      "4    [input_ids, attention_mask]   \n",
      "..                           ...   \n",
      "163  [input_ids, attention_mask]   \n",
      "164  [input_ids, attention_mask]   \n",
      "165  [input_ids, attention_mask]   \n",
      "166  [input_ids, attention_mask]   \n",
      "167  [input_ids, attention_mask]   \n",
      "\n",
      "                                                output  \\\n",
      "0    {'logits': [[tensor(-2.3454, grad_fn=<UnbindBa...   \n",
      "1    {'logits': [[tensor(-2.9243, grad_fn=<UnbindBa...   \n",
      "2    {'logits': [[tensor(-0.1904, grad_fn=<UnbindBa...   \n",
      "3    {'logits': [[tensor(-2.1626, grad_fn=<UnbindBa...   \n",
      "4    {'logits': [[tensor(-0.4185, grad_fn=<UnbindBa...   \n",
      "..                                                 ...   \n",
      "163  {'logits': [[tensor(-1.2227, grad_fn=<UnbindBa...   \n",
      "164  {'logits': [[tensor(-2.4629, grad_fn=<UnbindBa...   \n",
      "165  {'logits': [[tensor(-2.6766, grad_fn=<UnbindBa...   \n",
      "166  {'logits': [[tensor(-2.3950, grad_fn=<UnbindBa...   \n",
      "167  {'logits': [[tensor(-2.2264, grad_fn=<UnbindBa...   \n",
      "\n",
      "                                     scores  \n",
      "0      [0.012829757, 0.21647434, 0.7706959]  \n",
      "1    [0.0025204902, 0.059670344, 0.9378091]  \n",
      "2       [0.17661668, 0.74489474, 0.0784886]  \n",
      "3      [0.010495567, 0.11259531, 0.8769091]  \n",
      "4       [0.18982491, 0.6115974, 0.19857769]  \n",
      "..                                      ...  \n",
      "163       [0.07528579, 0.526805, 0.3979092]  \n",
      "164    [0.002622035, 0.022788968, 0.974589]  \n",
      "165   [0.0041091316, 0.06917517, 0.9267157]  \n",
      "166   [0.010198248, 0.18377367, 0.80602807]  \n",
      "167    [0.014538733, 0.25270712, 0.7327542]  \n",
      "\n",
      "[168 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,from_tf=True)\n",
    "\n",
    "# download label mapping\n",
    "# labels=[]\n",
    "# mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "# with urllib.request.urlopen(mapping_link) as f:\n",
    "#     html = f.read().decode('utf-8').split(\"\\n\")\n",
    "#     csvreader = csv.reader(html, delimiter='\\t')\n",
    "# labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "\n",
    "model.save_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(MODEL)\n",
    "\n",
    "comments['processed']=comments['Answer:'].apply(lambda x: preprocess(x))\n",
    "comments['encoded']=comments['processed'].apply(lambda x:tokenizer(x,return_tensors='pt'))\n",
    "comments['output']=comments['encoded'].apply(lambda x:model(**x))\n",
    "comments['scores']=comments['output'].apply(lambda x:softmax(x[0][0].detach().numpy()))\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load and preprocess data\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].map(lambda example: {\"text\": example[\"text\"], \"label\": example[\"label\"]}).shuffle(seed=42).select([i for i in list(range(500))])\n",
    "val_dataset = dataset[\"test\"].map(lambda example: {\"text\": example[\"text\"], \"label\": example[\"label\"]}).shuffle(seed=42).select([i for i in list(range(50))])\n",
    "print(dataset,'dataset')\n",
    "print(train_dataset,'train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True,max_length=300)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "\n",
    "# Define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"no\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_strategy = 'no'\n",
    ")\n",
    "\n",
    "def compute_accuracy(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_accuracy\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "trainer.save_model(\"trained-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/38 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\feryo\\OneDrive\\Documents\\GitHub\\FYP-GoodHealth\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                             \n",
      " 33%|███▎      | 3/9 [00:05<00:10,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13799996674060822, 'eval_runtime': 0.1235, 'eval_samples_per_second': 40.472, 'eval_steps_per_second': 8.094, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 67%|██████▋   | 6/9 [00:11<00:05,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05180676653981209, 'eval_runtime': 0.1649, 'eval_samples_per_second': 30.325, 'eval_steps_per_second': 6.065, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 9/9 [00:16<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03951571136713028, 'eval_runtime': 0.192, 'eval_samples_per_second': 26.045, 'eval_steps_per_second': 5.209, 'epoch': 3.0}\n",
      "{'train_runtime': 16.6532, 'train_samples_per_second': 6.846, 'train_steps_per_second': 0.54, 'train_loss': 0.49953216976589626, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load CSV file using pandas\n",
    "data = pd.read_csv(\"generated_comments.csv\")\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,from_tf=True)\n",
    "# Perform train/test split\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load tokenizer from Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"your_pretrained_tokenizer\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a dictionary of datasets\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Define Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.save_model('csv_batch16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/38 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\feryo\\OneDrive\\Documents\\GitHub\\FYP-GoodHealth\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                             \n",
      " 33%|███▎      | 3/9 [00:05<00:09,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13799896836280823, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.1592, 'eval_samples_per_second': 31.412, 'eval_steps_per_second': 6.282, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 67%|██████▋   | 6/9 [00:11<00:05,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05180616304278374, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.1498, 'eval_samples_per_second': 33.378, 'eval_steps_per_second': 6.676, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 9/9 [00:18<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039515234529972076, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.1513, 'eval_samples_per_second': 33.046, 'eval_steps_per_second': 6.609, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:20<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 20.4722, 'train_samples_per_second': 5.569, 'train_steps_per_second': 0.44, 'train_loss': 0.4995316399468316, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 87.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039515234529972076, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.2487, 'eval_samples_per_second': 20.104, 'eval_steps_per_second': 4.021, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned-model\\\\tokenizer_config.json',\n",
       " 'finetuned-model\\\\special_tokens_map.json',\n",
       " 'finetuned-model\\\\vocab.json',\n",
       " 'finetuned-model\\\\merges.txt',\n",
       " 'finetuned-model\\\\added_tokens.json',\n",
       " 'finetuned-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,from_tf=True)\n",
    "\n",
    "# Load CSV file using pandas\n",
    "data = pd.read_csv(\"generated_comments.csv\")\n",
    "\n",
    "# Perform train/test split\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load tokenizer from Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"your_pretrained_tokenizer\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a dictionary of datasets\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# Create a function to compute metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Define Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "trainer.save_model('finetuned-model')\n",
    "tokenizer.save_pretrained('finetuned-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[Ac:\\Users\\feryo\\OneDrive\\Documents\\GitHub\\FYP-GoodHealth\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                        \n",
      "\u001b[A                                  \n",
      "\n",
      "\n",
      " 33%|███▎      | 5/15 [00:44<00:12,  1.28s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.057017575949430466, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_weighted_f1': 1.0, 'eval_pos_f1': 1.0, 'eval_neg_f1': 1.0, 'eval_neu_f1': 1.0, 'eval_runtime': 0.1451, 'eval_samples_per_second': 34.462, 'eval_steps_per_second': 6.892, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "\u001b[A                                          \n",
      "\n",
      "\n",
      " 33%|███▎      | 5/15 [00:52<00:12,  1.28s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.027921680361032486, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_weighted_f1': 1.0, 'eval_pos_f1': 1.0, 'eval_neg_f1': 1.0, 'eval_neu_f1': 1.0, 'eval_runtime': 0.1464, 'eval_samples_per_second': 34.162, 'eval_steps_per_second': 6.832, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "\u001b[A                                          \n",
      "\n",
      "\n",
      " 33%|███▎      | 5/15 [01:01<00:12,  1.28s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.028729181736707687, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_weighted_f1': 1.0, 'eval_pos_f1': 1.0, 'eval_neg_f1': 1.0, 'eval_neu_f1': 1.0, 'eval_runtime': 0.1558, 'eval_samples_per_second': 32.091, 'eval_steps_per_second': 6.418, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      " 33%|███▎      | 5/15 [01:03<00:12,  1.28s/it]\n",
      "100%|██████████| 15/15 [00:26<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 26.1413, 'train_samples_per_second': 4.361, 'train_steps_per_second': 0.574, 'train_loss': 0.42616421381632485, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.027921680361032486, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_weighted_f1': 1.0, 'eval_pos_f1': 1.0, 'eval_neg_f1': 1.0, 'eval_neu_f1': 1.0, 'eval_runtime': 0.146, 'eval_samples_per_second': 34.258, 'eval_steps_per_second': 6.852, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,from_tf=True)\n",
    "# Load CSV file using pandas\n",
    "data = pd.read_csv(\"generated_comments.csv\")\n",
    "\n",
    "# Perform train/test split\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load tokenizer from Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"your_pretrained_tokenizer\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a dictionary of datasets\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# Create a function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "    pos_f1 = f1_score(labels == 0, preds == 0, average='weighted')\n",
    "    neg_f1 = f1_score(labels == 1, preds == 1, average='weighted')\n",
    "    neu_f1 = f1_score(labels == 2, preds == 2, average='weighted')\n",
    "    weighted_f1 = (pos_f1 + neg_f1 + neu_f1) / 3\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'pos_f1': pos_f1,\n",
    "        'neg_f1': neg_f1,\n",
    "        'neu_f1': neu_f1,\n",
    "    }\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.0011,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Define Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "trainer.save_model('csv_batch8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[Ac:\\Users\\feryo\\OneDrive\\Documents\\GitHub\\FYP-GoodHealth\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                              \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                            \n",
      "\u001b[A                                          \n",
      "\n",
      " 33%|███▎      | 5/15 [01:28<00:12,  1.28s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6665213108062744, 'eval_runtime': 0.1813, 'eval_samples_per_second': 49.632, 'eval_steps_per_second': 5.515, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                              \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                            \n",
      "\u001b[A                                          \n",
      "\n",
      " 33%|███▎      | 5/15 [01:39<00:12,  1.28s/it] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7316714525222778, 'eval_runtime': 0.18, 'eval_samples_per_second': 49.987, 'eval_steps_per_second': 5.554, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                              \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                            \n",
      "\u001b[A                                          \n",
      "\n",
      " 33%|███▎      | 5/15 [01:52<00:12,  1.28s/it] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                              \n",
      "\u001b[A                                           \n",
      "\n",
      " 33%|███▎      | 5/15 [01:52<00:12,  1.28s/it] \n",
      "100%|██████████| 15/15 [00:36<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7393313050270081, 'eval_runtime': 0.1829, 'eval_samples_per_second': 49.204, 'eval_steps_per_second': 5.467, 'epoch': 3.0}\n",
      "{'train_runtime': 36.1322, 'train_samples_per_second': 6.642, 'train_steps_per_second': 0.415, 'train_loss': 0.5734076182047526, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,from_tf=True)\n",
    "# Load CSV file using pandas\n",
    "data = pd.read_excel(\"generated_comments.xlsx\")\n",
    "\n",
    "# Perform train/test split\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load tokenizer from Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"your_pretrained_tokenizer\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a dictionary of datasets\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Define Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.save_model('excel_batch16')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b9cd609fe250df32380b10f589404d8f31c7e924815d1cf5d2fd9dff4822a00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
