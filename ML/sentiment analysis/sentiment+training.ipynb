{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comments=pd.read_excel('evaluations_overall_comments.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Target:                      Evaluator:       Rotation Dates:  \\\n",
      "0     Eliza Chen      Dr. Ong, Andrew Ming Liang    7/1/2022-7/31/2022   \n",
      "1     Eliza Chen         Dr. Chan, Webber Pak Wo    7/1/2022-7/31/2022   \n",
      "2     Eliza Chen              Dr. Lim, Chee Hooi    7/1/2022-7/31/2022   \n",
      "3     Eliza Chen         Dr. Chang, Jason Pik Eu    8/1/2022-8/31/2022   \n",
      "4     Eliza Chen              Dr. Tan, Chee Kiat    8/1/2022-8/31/2022   \n",
      "..           ...                             ...                   ...   \n",
      "163  Rachel Yeap     Dr. Tan, Malcolm Teck Kiang    7/1/2022-7/31/2022   \n",
      "164  Rachel Yeap  Dr. Khor, Christopher Jen Lock    8/1/2022-8/31/2022   \n",
      "165  Rachel Yeap               Dr. Liou, Wei Lun    8/1/2022-8/31/2022   \n",
      "166  Rachel Yeap             Dr. Loo, Khang Ning    9/1/2022-10/2/2022   \n",
      "167  Rachel Yeap        Dr. Kwek, Andrew Boon Eu  10/3/2022-10/31/2022   \n",
      "\n",
      "                         Service:  \\\n",
      "0                             - -   \n",
      "1       SHS-GASTRO:BASIC ENDO-SGH   \n",
      "2       SHS-GASTRO:BASIC ENDO-SGH   \n",
      "3       SHS-GASTRO:BASIC ENDO-SGH   \n",
      "4       SHS-GASTRO:BASIC ENDO-SGH   \n",
      "..                            ...   \n",
      "163  SHS-GASTRO:GEN GASTROHEP-SGH   \n",
      "164  SHS-GASTRO:GEN GASTROHEP-SGH   \n",
      "165  SHS-GASTRO:GEN GASTROHEP-SGH   \n",
      "166            SHS-IM:GEN MED-SGH   \n",
      "167       SHS-GASTRO:ADV ENDO-CGH   \n",
      "\n",
      "                                               Answer:  \\\n",
      "0    Eliza Chen is starting out and therefore as ex...   \n",
      "1    As a new senior resident to gastroenterology, ...   \n",
      "2                                           No concern   \n",
      "3    Overall I would rate Eliza Chen highly as a SR...   \n",
      "4    Eliza Chen has a quiet disposition and gives g...   \n",
      "..                                                 ...   \n",
      "163  did a week of bleeder call, good mgmt plans an...   \n",
      "164  very competent, extremely professional, and a ...   \n",
      "165  No issue. Making good progress compared to fir...   \n",
      "166  Rachel is clinically competent and I can trust...   \n",
      "167  Able to summarise key issues and suggest a rea...   \n",
      "\n",
      "                                             processed  \\\n",
      "0    Eliza Chen is starting out and therefore as ex...   \n",
      "1    As a new senior resident to gastroenterology, ...   \n",
      "2                                           No concern   \n",
      "3    Overall I would rate Eliza Chen highly as a SR...   \n",
      "4    Eliza Chen has a quiet disposition and gives g...   \n",
      "..                                                 ...   \n",
      "163  did a week of bleeder call, good mgmt plans an...   \n",
      "164  very competent, extremely professional, and a ...   \n",
      "165  No issue. Making good progress compared to fir...   \n",
      "166  Rachel is clinically competent and I can trust...   \n",
      "167  Able to summarise key issues and suggest a rea...   \n",
      "\n",
      "                         encoded  \\\n",
      "0    [input_ids, attention_mask]   \n",
      "1    [input_ids, attention_mask]   \n",
      "2    [input_ids, attention_mask]   \n",
      "3    [input_ids, attention_mask]   \n",
      "4    [input_ids, attention_mask]   \n",
      "..                           ...   \n",
      "163  [input_ids, attention_mask]   \n",
      "164  [input_ids, attention_mask]   \n",
      "165  [input_ids, attention_mask]   \n",
      "166  [input_ids, attention_mask]   \n",
      "167  [input_ids, attention_mask]   \n",
      "\n",
      "                                                output  \\\n",
      "0    {'logits': [[tensor(-2.3454, grad_fn=<UnbindBa...   \n",
      "1    {'logits': [[tensor(-2.9243, grad_fn=<UnbindBa...   \n",
      "2    {'logits': [[tensor(-0.1904, grad_fn=<UnbindBa...   \n",
      "3    {'logits': [[tensor(-2.1626, grad_fn=<UnbindBa...   \n",
      "4    {'logits': [[tensor(-0.4185, grad_fn=<UnbindBa...   \n",
      "..                                                 ...   \n",
      "163  {'logits': [[tensor(-1.2227, grad_fn=<UnbindBa...   \n",
      "164  {'logits': [[tensor(-2.4629, grad_fn=<UnbindBa...   \n",
      "165  {'logits': [[tensor(-2.6766, grad_fn=<UnbindBa...   \n",
      "166  {'logits': [[tensor(-2.3950, grad_fn=<UnbindBa...   \n",
      "167  {'logits': [[tensor(-2.2264, grad_fn=<UnbindBa...   \n",
      "\n",
      "                                     scores  \n",
      "0      [0.012829757, 0.21647434, 0.7706959]  \n",
      "1    [0.0025204902, 0.059670344, 0.9378091]  \n",
      "2       [0.17661668, 0.74489474, 0.0784886]  \n",
      "3       [0.010495567, 0.1125953, 0.8769091]  \n",
      "4       [0.18982491, 0.6115974, 0.19857769]  \n",
      "..                                      ...  \n",
      "163       [0.07528579, 0.526805, 0.3979092]  \n",
      "164    [0.002622035, 0.022788968, 0.974589]  \n",
      "165   [0.0041091316, 0.06917517, 0.9267157]  \n",
      "166   [0.010198248, 0.18377367, 0.80602807]  \n",
      "167    [0.014538733, 0.25270712, 0.7327542]  \n",
      "\n",
      "[168 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiff2/twitter-roberta-base-{task}\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,from_tf=True)\n",
    "\n",
    "# download label mapping\n",
    "# labels=[]\n",
    "# mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "# with urllib.request.urlopen(mapping_link) as f:\n",
    "#     html = f.read().decode('utf-8').split(\"\\n\")\n",
    "#     csvreader = csv.reader(html, delimiter='\\t')\n",
    "# labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "\n",
    "model.save_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(MODEL)\n",
    "\n",
    "comments['processed']=comments['Answer:'].apply(lambda x: preprocess(x))\n",
    "comments['encoded']=comments['processed'].apply(lambda x:tokenizer(x,return_tensors='pt'))\n",
    "comments['output']=comments['encoded'].apply(lambda x:model(**x))\n",
    "comments['scores']=comments['output'].apply(lambda x:softmax(x[0][0].detach().numpy()))\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/WongQiHuiYve/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb8bfe241af430a94ddb597910f38a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\WongQiHuiYve\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-b3cb4622bfc17443.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\WongQiHuiYve\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-487ff0ef3ae734af.arrow\n",
      "Loading cached processed dataset at C:\\Users\\WongQiHuiYve\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-d07e09006a541566.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\WongQiHuiYve\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-132cf935f76d806f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "}) dataset\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 500\n",
      "}) train_dataset\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load and preprocess data\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].map(lambda example: {\"text\": example[\"text\"], \"label\": example[\"label\"]}).shuffle(seed=42).select([i for i in list(range(500))])\n",
    "val_dataset = dataset[\"test\"].map(lambda example: {\"text\": example[\"text\"], \"label\": example[\"label\"]}).shuffle(seed=42).select([i for i in list(range(50))])\n",
    "print(dataset,'dataset')\n",
    "print(train_dataset,'train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d2e2f924634df0bb9890fb0628534f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13371d1ec15f48dd9bd16437ff562d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True,max_length=300)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78dc9fc4a4a4239a7f9f4987e90c8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de818484bfca4612bd469211497adb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\WongQiHuiYve\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 189\n",
      "  Number of trainable parameters = 124647939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aef3edf3e514b7cbc60bf9012fd92f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5057, 'learning_rate': 1e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to trained-model\n",
      "Configuration saved in trained-model\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 9256.0139, 'train_samples_per_second': 0.162, 'train_steps_per_second': 0.02, 'train_loss': 0.38083001797792143, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in trained-model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"no\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_strategy = 'no'\n",
    ")\n",
    "\n",
    "def compute_accuracy(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_accuracy\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "trainer.save_model(\"trained-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "dataset = load_dataset(\"csv\", data_files=\"generated_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653903d2b1c641a9b2657e68c8b62368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe269b02a9134585a7934bd9eeb1d6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\WongQiHuiYve\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 38\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9\n",
      "  Number of trainable parameters = 124647939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09c7c03f6bd4764979bd20d203535d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ef9b15e3854db0bef9e0b1a3773657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13799993693828583, 'eval_runtime': 0.5757, 'eval_samples_per_second': 8.685, 'eval_steps_per_second': 1.737, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad7668853dc4c1682069fd53795f0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05180678889155388, 'eval_runtime': 0.3894, 'eval_samples_per_second': 12.84, 'eval_steps_per_second': 2.568, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1286eb79c65d4eafb19c967a2fe47d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03951571136713028, 'eval_runtime': 0.3629, 'eval_samples_per_second': 13.777, 'eval_steps_per_second': 2.755, 'epoch': 3.0}\n",
      "{'train_runtime': 58.6702, 'train_samples_per_second': 1.943, 'train_steps_per_second': 0.153, 'train_loss': 0.49953216976589626, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=0.49953216976589626, metrics={'train_runtime': 58.6702, 'train_samples_per_second': 1.943, 'train_steps_per_second': 0.153, 'train_loss': 0.49953216976589626, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load CSV file using pandas\n",
    "data = pd.read_csv(\"generated_comments.csv\")\n",
    "\n",
    "# Perform train/test split\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load tokenizer from Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"your_pretrained_tokenizer\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a dictionary of datasets\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Define Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to self-labelled-trained\n",
      "Configuration saved in self-labelled-trained\\config.json\n",
      "Model weights saved in self-labelled-trained\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('self-labelled-trained')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
