{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT ANALYSIS TO QA PIPELINE\n",
    "# to find out the reason for a sentiment\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis and question answering models\n",
    "sa_model = pipeline(\"sentiment-analysis\")\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "\n",
    "# Review to analyze\n",
    "review = \"I had a great experience with this product. The customer service was excellent and the product exceeded my expectations.\"\n",
    "\n",
    "# Perform sentiment analysis on the review\n",
    "sentiment = sa_model(review)[0]['label']\n",
    "\n",
    "# Extract key phrases from the review\n",
    "key_phrases = ['customer service', 'product', 'expectations']\n",
    "\n",
    "# Generate questions based on the sentiment and key phrases\n",
    "if sentiment == 'POSITIVE':\n",
    "    questions = ['What made the customer service excellent?',\n",
    "                 'What features of the product exceeded your expectations?']\n",
    "elif sentiment == 'NEGATIVE':\n",
    "    questions = ['What issues did you encounter with the customer service?',\n",
    "                 'What aspects of the product were disappointing?']\n",
    "else:\n",
    "    questions = ['What can you say about the customer service?',\n",
    "                 'What can you say about the product?']\n",
    "\n",
    "# Use QA to answer the questions\n",
    "answers = []\n",
    "for question in questions:\n",
    "    answer = qa_model(question=question, context=review)\n",
    "    answers.append(answer['answer'])\n",
    "\n",
    "# Print the sentiment and answers\n",
    "print('Sentiment:', sentiment)\n",
    "print('Answers:', answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC MODELING\n",
    "\n",
    "!pip install sentence-transformers  # Install the required library\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load BioBERT model\n",
    "biobert = SentenceTransformer('gsarti/biobert-nli')\n",
    "\n",
    "# Load the text data\n",
    "df = pd.read_csv('path/to/data.csv')\n",
    "\n",
    "# Convert the text data to sentence embeddings\n",
    "sentences = df['text'].tolist()\n",
    "embeddings = biobert.encode(sentences)\n",
    "\n",
    "# Perform topic modeling using Latent Dirichlet Allocation\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(embeddings)\n",
    "\n",
    "# Print the topics and the top words in each topic\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print('Topic %d:' % (idx))\n",
    "    print(' '.join([biobert.decode([feature]) for feature in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WongQiHuiYve\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# KEYWORD EXTRACTION\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input \n",
    "      \"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "print(type(distances))\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(len(distances[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comments=pd.read_excel('comments.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WongQiHuiYve\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "\n",
    "sentences=comments['Comments'].values.tolist()\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit(sentences)\n",
    "candidates = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "doc_embedding\n",
    "comments['Doc_Embed']=comments.apply(lambda x: model.encode([x['Comments']]),axis=1)\n",
    "comments['Distance']=comments.apply(lambda x:cosine_similarity(x['Doc_Embed'],candidate_embeddings),axis=1)\n",
    "top_n = 5\n",
    "# distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "# keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03689715]\n",
      " [ 0.00377444]\n",
      " [ 0.21624918]\n",
      " [-0.0212972 ]\n",
      " [ 0.03549244]\n",
      " [ 0.12607025]\n",
      " [ 0.01167195]]\n",
      "[[0.3550511 ]\n",
      " [0.30662364]\n",
      " [0.4605971 ]\n",
      " [0.36754215]\n",
      " [0.3787705 ]\n",
      " [0.5830076 ]\n",
      " [0.3822926 ]]\n",
      "[[0.06624319]\n",
      " [0.08109218]\n",
      " [0.26776776]\n",
      " [0.07373106]\n",
      " [0.14842369]\n",
      " [0.26374188]\n",
      " [0.11118844]]\n",
      "[[0.38388085]\n",
      " [0.3735308 ]\n",
      " [0.51662016]\n",
      " [0.37933713]\n",
      " [0.32125825]\n",
      " [0.6105641 ]\n",
      " [0.48755568]]\n",
      "[[0.07437953]\n",
      " [0.10615816]\n",
      " [0.2765436 ]\n",
      " [0.08318146]\n",
      " [0.13505548]\n",
      " [0.25954688]\n",
      " [0.10121397]]\n",
      "[[0.37491083]\n",
      " [0.46937156]\n",
      " [0.5838687 ]\n",
      " [0.3889848 ]\n",
      " [0.40230277]\n",
      " [0.4974962 ]\n",
      " [0.50688106]]\n",
      "[[0.01981166]\n",
      " [0.02514253]\n",
      " [0.2078799 ]\n",
      " [0.04686979]\n",
      " [0.11466895]\n",
      " [0.22064799]\n",
      " [0.07252524]]\n",
      "[[0.51451814]\n",
      " [0.52096224]\n",
      " [0.5859443 ]\n",
      " [0.5054382 ]\n",
      " [0.47318393]\n",
      " [0.6962565 ]\n",
      " [0.57322437]]\n",
      "[[0.11411806]\n",
      " [0.12221289]\n",
      " [0.30409703]\n",
      " [0.13269892]\n",
      " [0.18024251]\n",
      " [0.32534862]\n",
      " [0.13431072]]\n",
      "[[0.32242846]\n",
      " [0.3150214 ]\n",
      " [0.44536775]\n",
      " [0.29361004]\n",
      " [0.29850075]\n",
      " [0.46683782]\n",
      " [0.35305566]]\n",
      "[[0.14306274]\n",
      " [0.1740191 ]\n",
      " [0.33708614]\n",
      " [0.18439011]\n",
      " [0.20181513]\n",
      " [0.29816025]\n",
      " [0.18609661]]\n",
      "[[0.04954749]\n",
      " [0.05709334]\n",
      " [0.17942946]\n",
      " [0.15758964]\n",
      " [0.1561882 ]\n",
      " [0.1929233 ]\n",
      " [0.1631096 ]]\n",
      "[[0.185888  ]\n",
      " [0.21669284]\n",
      " [0.28609958]\n",
      " [0.29379207]\n",
      " [0.21276277]\n",
      " [0.25845036]\n",
      " [0.30833495]]\n",
      "[[0.10542175]\n",
      " [0.11676066]\n",
      " [0.25829193]\n",
      " [0.20363349]\n",
      " [0.18861985]\n",
      " [0.2561105 ]\n",
      " [0.22217008]]\n",
      "[[0.05771157]\n",
      " [0.0636661 ]\n",
      " [0.17661661]\n",
      " [0.1594586 ]\n",
      " [0.13603929]\n",
      " [0.18642485]\n",
      " [0.19839132]]\n",
      "[[ 0.00660942]\n",
      " [-0.03124523]\n",
      " [ 0.16667686]\n",
      " [ 0.10531612]\n",
      " [ 0.0769971 ]\n",
      " [ 0.15126908]\n",
      " [ 0.08712412]]\n",
      "[[0.08401771]\n",
      " [0.07820674]\n",
      " [0.19834067]\n",
      " [0.17741427]\n",
      " [0.14731881]\n",
      " [0.22072162]\n",
      " [0.2095009 ]]\n",
      "[[0.02312277]\n",
      " [0.04410268]\n",
      " [0.12141802]\n",
      " [0.08020818]\n",
      " [0.09288241]\n",
      " [0.13980971]\n",
      " [0.07291382]]\n",
      "[[0.0877331 ]\n",
      " [0.10276437]\n",
      " [0.27926648]\n",
      " [0.11929273]\n",
      " [0.16876215]\n",
      " [0.29068768]\n",
      " [0.0792489 ]]\n",
      "[[0.33432418]\n",
      " [0.2924748 ]\n",
      " [0.48943394]\n",
      " [0.35750794]\n",
      " [0.3268959 ]\n",
      " [0.6102259 ]\n",
      " [0.36966705]]\n"
     ]
    }
   ],
   "source": [
    "comments['Keywords']=comments['Distance'].apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "1     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "2     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "3     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "4     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "5     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "6     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "7     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "8     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "9     [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "10    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "11    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "12    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "13    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "14    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "15    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "16    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "17    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "18    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "19    [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...\n",
       "Name: Keywords, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "comments['a']=comments['Distance'].apply(lambda x: print(type(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Assume that documents is a list of text documents\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Initialize the NMF object with 100 components\n",
    "nmf = NMF(n_components=100, init='nndsvd', max_iter=200)\n",
    "\n",
    "# Factorize the term-document matrix into a word-topic matrix and a topic-document matrix\n",
    "W = nmf.fit_transform(tfidf)\n",
    "\n",
    "# The resulting word embeddings are the rows of the word-topic matrix\n",
    "word_embeddings = W.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
